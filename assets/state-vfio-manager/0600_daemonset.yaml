apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nvidia-vfio-manager
  namespace: "FILLED BY THE OPERATOR"
  labels:
    app: nvidia-vfio-manager
spec:
  selector:
    matchLabels:
      name: nvidia-vfio-manager
  template:
    metadata:
      labels:
        name: nvidia-vfio-manager
    spec:
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      nodeSelector:
        nvidia.com/gpu.deploy.vfio-manager: "true"
      priorityClassName: system-node-critical
      serviceAccountName: nvidia-vfio-manager
      initContainers:
        - name: setup-udev-rules
          image: "nvcr.io/nvidia/cloud-native/k8s-driver-manager:v0.9.1"
          imagePullPolicy: IfNotPresent
          command: ["/bin/sh", "-c"]
          args:
            - |
              cat > /host/etc/udev/rules.d/vfio-passthrough.rules <<'EOF'
              # /dev/vfio/**
              SUBSYSTEM=="vfio", OWNER="root", GROUP="kvm"

              # /dev/iommu
              SUBSYSTEM=="misc", KERNEL=="iommu", OWNER="root", GROUP="kvm", MODE="0666"

              # /dev/vfio/devices/vfio**
              SUBSYSTEM=="vfio-dev", OWNER="root", GROUP="kvm", MODE="0666"
              EOF
              chroot /host udevadm control --reload-rules
              chroot /host udevadm trigger
          env:
          - name: NVIDIA_VISIBLE_DEVICES
            value: void
          securityContext:
            privileged: true
          volumeMounts:
            - name: host-root
              mountPath: /host
              mountPropagation: HostToContainer
        - name: k8s-driver-manager
          image: "FILLED BY THE OPERATOR"
          imagePullPolicy: IfNotPresent
          command: ["driver-manager"]
          args: ["uninstall_driver"]
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          # always use runc for driver containers
          - name: NVIDIA_VISIBLE_DEVICES
            value: void
          - name: ENABLE_GPU_POD_EVICTION
            value: "false"
          - name: ENABLE_AUTO_DRAIN
            value: "false"
          - name: OPERATOR_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          securityContext:
            privileged: true
          volumeMounts:
            - name: run-nvidia
              mountPath: /run/nvidia
              mountPropagation: Bidirectional
            - name: host-root
              mountPath: /host
              readOnly: true
              mountPropagation: HostToContainer
            - name: host-sys
              mountPath: /sys
      containers:
        - name: nvidia-vfio-manager
          image: "FILLED BY THE OPERATOR"
          imagePullPolicy: IfNotPresent
          command: ["/bin/sh", "-c"]
          args:
            - /bin/vfio-manage.sh bind --all && sleep inf
          resources:
            limits:
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 200Mi
          volumeMounts:
          - name: nvidia-vfio-manager
            readOnly: true
            mountPath: /bin/vfio-manage.sh
            subPath: vfio-manage.sh
          - name: host-sys
            mountPath: /sys
          - name: host-root
            mountPath: /host
          securityContext:
            privileged: true
            seLinuxOptions:
              level: "s0"
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh", "-c", "/bin/vfio-manage.sh unbind --all"]
      terminationGracePeriodSeconds: 30
      volumes:
        - name: nvidia-vfio-manager
          configMap:
            name: nvidia-vfio-manager
            defaultMode: 448
        - name: host-sys
          hostPath:
            path: /sys
            type: Directory
        - name: run-nvidia
          hostPath:
            path: /run/nvidia
            type: DirectoryOrCreate
        - name: host-root
          hostPath:
            path: "/"
